

弱监督时序动作定位任务从只含有视频级分类标签的未修剪视频中学习，希望掌握动作模式并准确地找出动作实例。已有算法普遍受“动作背景混淆”问题的困扰，近期，一些工作提出**动作单帧标注**的框架来处理这种难题。与传统弱监督时序动作定位算法相比，基于动作单帧标注的算法只消耗相似的标注成本，却能够稳定地提升动作定位性能。同时，我们在研究中发现已有时序动作定位算法可以准确地发掘到相当一部分动作帧，而性能瓶颈主要在于背景错误。因此，我们将动作单帧标注发展为**背景单帧标注**，并设计两个模块来协同地挖掘标注帧的位置信息和特征信息，以准确地学习动作模式。在挖掘位置信息时，我们不仅在标注帧进行有监督的分类学习，也提出一个新颖的得分分离模块来增加动作帧和背景帧响应的差异性。在挖掘特征信息时，我们提出亲和力模块来衡量每帧和它邻近帧的相似性，以帮助每帧来动态地计算时序卷积，并获取更准确的动作模式。我们在三个基准数据集上进行实验，验证了所提方法具有更优异的时序动作定位性能，同时证明了背景单帧标注的合理性和有效性



 **时序动作定位（Temporal Action Localization）**





时序动作定位 (Temporal Action Localization) 也称为时序动作检测 (Temporal Action Detection)，是视频理解的另一个重要领域。动作识别可以看作是一个纯分类问题，其中要识别的视频基本上已经过剪辑(Trimmed)，即每个视频包含一段明确的动作，视频时长较短，且有唯一确定的动作类别。而在时序动作定位领域，视频通常没有被剪辑(Untrimmed)，视频时长较长，动作通常只发生在视频中的一小段时间内，视频可能包含多个动作，也可能不包含动作，即为背景(Background) 类。时序动作定位不仅要预测视频中包含了什么动作，还要预测动作的起始和终止时刻。相比于动作识别，时序动作定位更接近现实场景。





时序动作定位可以看作由两个子任务组成，一个子任务是预测动作的起止时序区间，另一个子任务是预测动作的类别。由于动作识别领域经过近年来的发展，预测动作类别的算法逐渐成熟，因此时序动作定位的关键是预测动作的起止时序区间，有不少研究工作专注于该子任务，ActivityNet 竞赛除了每年举办时序动作定位竞赛，还专门组织候选时序区间生成竞赛(也称为时序动作区间提名)。





既然要预测动作的起止区间，一种最朴素的想法是穷举所有可能的区间，然后逐一判断该区间内是否包含动作。对于一个 T 帧的视频，所有可能的区间为 ，穷举所有的区间会带来非常庞大的计算量。





时序动作检测的很多思路源于图像目标检测 (Object Detection)，了解目标检测的一些常见算法和关键思路对学习时序动作定位很有帮助。相比于图像分类的目标是预测图像中物体的类别，目标检测不仅要预测类别，还要预测出物体在图像中的空间位置信息，以物体外接矩形的包围盒(Bounding Box) 形式表示。





**3.1 基于滑动窗的算法**





这类算法的基本思路是预先定义一系列不同时长的滑动窗，之后滑动窗在视频上沿着时间维度进行滑动，并逐一判断每个滑动窗对应的时序区间内具体是什么动作类别。图 6 (a) 中使用了 3 帧时长的滑动窗，图 6 (b) 中使用了 5 帧时长的滑动窗，最终汇总不同时长的滑动窗的类别预测结果。可以知道，该视频中包含的动作是悬崖跳水、动作出现的起止时序区间在靠近视频结尾的位置。



![img](https://pics1.baidu.com/feed/adaf2edda3cc7cd9f2cdcdd3d3676536b90e914c.jpeg?token=5be5497cd33d2541955e63e38e9a8f1f)



*图 6：基于滑动窗的算法流程图。本图源于《深度学习视频理解》*





如果对目标检测熟悉的读者可以联想到，Viola-Jones 实时人脸检测器 (Viola & Jones, 2004) 中也采用了滑动窗的思想，其先用滑动窗在图像上进行密集滑动，之后提取每个滑动窗对应的图像区域的特征，最后通过 AdaBoost 级联分类器进行分类。Viola-Jones 实时人脸检测器是计算机视觉历史上具有里程碑意义的算法之一，获得了 2011 年 CVPR(Computer Vision and Pattern Recognition，计算机视觉和模式识别)大会用于表彰十年影响力的 Longuet-Higgins 奖。





**3.2 基于候选时序区间的算法**





目标检测算法中的两阶段 (Two-Stage) 算法将目标检测分为两个阶段: 第一阶段产生图像中可能存在目标 的候选区域(Region Proposal)，一般一张图像可以产生成百上千个候选区域，这一阶段和具体的类别无关; 第二阶段逐一判断每个候选区域的类别并对候选区域的边界进行修正。





类比于两阶段的目标检测算法，基于候选时序区间的时序动作定位算法也将整个过程分为两个阶段: 第一阶段产生视频中动作可能发生的候选时序区间; 第 二阶段逐一判断每个候选时序区间的类别并对候选时序区间的边界进行修正。最终将两个阶段的预测结果结合起来，得到未被剪辑视频中动作的类别和起止时刻预测。



![img](https://pics4.baidu.com/feed/3b292df5e0fe9925eab84018d7ce1ad68cb171b5.jpeg?token=6def8adf1c3087ffdb2bc38d40c085e9)



*图 7：Faster R-CNN 和基于候选时序区间的方法类比。本图源于《深度学习视频理解》*





**3.3 自底向上的时序动作定位算法**





基于滑动窗和基于候选时序区间的时序动作定位算法都可以看作是自顶向下的算法，其本质是预先定义好一系列不同时长的滑动窗或锚点时序区间，之后判断每个滑动窗位置或锚点时序区间是否包含动作并对边界进行微调以产生候选时序区间。这类自顶向下的算法产生的候选时序区间会受到预先定义的滑动窗或锚点时序区间的影响，导致产生的候选时序区间不够灵活，区间的起止位置不够精确。





本节介绍自底向上的时序动作定位算法，这类算法首先局部预测视频动作开始和动作结束的时刻，之后将开始和结束时刻组合成候选时序区间，最后对每个候选时序区间进行类别预测。相比于自顶向下的算法，自底向上的算法预测的候选时序区间边界更加灵活。了解人体姿态估计 (Human Pose Estimation) 的读者可以联想到，人体姿态估计也可以分为自顶向下和自底向上两类算法，其中自顶 向下的算法先检测出人的包围盒，之后对每个包围盒内检测人体骨骼关键点，如 (Chen et al., 2018) 等; 自底向上的算法先检测所有的人体骨骼关键点，之后再组合成人，如 (Cao et al., 2021) 等。





BSN(Boundary Sensitive Network，边界敏感网络)(Lin et al., 2018b)是自底向上的时序动作定位算法的一个实例，BSN 获得了 2018 年 ActivityNet 时序动作定位竞赛的冠军和百度综艺节目精彩片段预测竞赛的冠军。



![img](https://pics6.baidu.com/feed/962bd40735fae6cdf5a0b988e5d54b2d43a70fb6.jpeg?token=23e1ee01a4a276e2898babd7b13d5431)



*图 8：BSN 网络结构图。本图源于《深度学习视频理解》*





**3.4 对时序结构信息建模的算法**





假设我们的目标是识别视频中的体操单跳 (Tumbling) 动作和对应的动作起止区间，见图 9 中的绿色框。图 9 中的蓝色框表示模型预测的候选时序区间，有的候选时序区间时序上并不完整，即候选时序区间并没有覆盖动作完整的起止过程。图 9 上半部分的算法直接基于候选时序区间内的特征对候选时序区间内的动作类别进行预测，导致模型一旦发现任何和单跳动作有关的视频片段，就会输出很高的置信度，进而导致时序定位不够精准。



![img](https://pics3.baidu.com/feed/5ab5c9ea15ce36d390205c49dc957e8ee950b117.jpeg?token=f8c4e4639ae70045ce0e833747e07b5b)



*图 9：SSN 对动作不同的阶段进行建模。本图源于(Zhao et al., 2020)*





SSN(Structured Segment Network，结构化视频段网络)算法 (Zhao et al., 2020) 对动作不同的阶段 (开始、过程、结束) 进行建模，SSN 不仅会预测候选时序区间内的动作类别，还会预测候选时序区间的完整性，这样做的好处是可以更好地定位动作开始和结束的时刻，SSN 只在候选时序区间和动作真实起止区间对齐的时候输出高置信度。





**3.5 逐帧预测的算法**





我们希望模型对动作时序区间的预测能够尽量精细。CDC (Convolutional-De-Convolutional networks，卷积 - 反卷积网络)算法 (Shou et al., 2017) 和前文介绍的其他算法的不同之处在于，CDC 可以对未被剪辑的视频逐帧预测动作的类别，这种预测粒度十分精细，使得对动作时序区间边界的定位更加精确。





如图 10 所示，输入一个未被剪辑的视频，首先利用动作识别网络提取视频特征，之后利用多层 CDC 层同时对特征进行空间维度的下采样和时间维度的上采样，进而得到视频中每帧的预测结果，最后结合候选时序区间得到动作类别和起止时刻的预测。CDC 的一个优点是预测十分高效，在单 GPU 服务器下，可以达到 500 FPS(Frames per Second，帧每秒)的预测速度。



![img](https://pics3.baidu.com/feed/622762d0f703918f763d27b8b45b629e58eec440.jpeg?token=bdf6b8867d5e0e01c1dba432e3f0ff47)



*图 10：CDC 网络结构图。本图源于《深度学习视频理解》*





**3.6 单阶段算**





目标检测算法可以大致分为两大类，其中一大类算法为两阶段算法，两阶段算法会先从图像中预测可能存在目标的候选区域，之后逐一判断每个候选区域的类别，并对候选区域边界进行修正。时序动作定位中也有一些算法采用了两阶段算法的策略，先从视频中预测可能包含动作的候选时序区间，之后逐一判断每个候选时序区间的类别，并对候选时序区间的边界进行修正，这部分算法已在 3.2 节介绍过。





另一大类算法为单阶段 (One-Stage) 算法，单阶段算法没有单独的候选区域生成的步骤，直接从图像中预测。在目标检测领域中，通常两阶段算法识别精度高，但是预测速度慢，单阶段算法识别精度略低，但是预测速度快。时序动作定位中也有一些算法采用了单阶段算法的策略。





到此为止，我们了解了许多时序动作定位算法，一种直观的想法是预先定义一组不同时长的滑动窗，之后滑动窗在视频上进行滑动，并逐一判断每个滑动窗对应的时序区间内的动作类别，如 S-CNN。TURN 和 CBR 以视频单元作为最小计算单位避免了滑动窗带来的冗余计算，并且可以对时序区间的边界进行修正; 受两阶段目标检测算法的启发，基于候选时序区间的算法先从视频中产生一些可能包含动作的候选时序区间，之后逐一判断每个候选时序区间内的动作类别，并对区间边界进行修正，如 R-C3D 和 TAL-Net; 自底向上的时序动作定位算法先预测动作开始和结束的时刻，之后将开始和结束时刻组合为候选时序区间，如 BSN、TSA-Net 和 BMN;SSN 不仅会预测每个区间的动作类别，还会 预测区间的完整性; CDC 通过卷积和反卷积操作可以逐帧预测动作类别。此外，单阶段目标检测的思路也可以用于时序动作定位中，如 SSAD、SS-TAD 和 GTAN。



![img](https://pics1.baidu.com/feed/f11f3a292df5e0fe56bd1ed5bf0670a15fdf7229.jpeg?token=b3b9af5cb709e7b44f5cce65578099a9)



*图 11：时序动作定位算法。本图源于《深度学习视频理解》*



